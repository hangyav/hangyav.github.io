---
title: 'LLM Sensitivity Challenges in Abusive Language Detection: Instruction-Tuned
  vs. Human Feedback'
authors:
- Yaqi Zhang
- Viktor Hangya
- Alexander Fraser
date: '2025-01-01'
publishDate: '2025-05-17T06:13:34.506387Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 31st International Conference on Computational Linguistics*'
abstract: The capacity of large language models (LLMs) to understand and distinguish
  socially unacceptable texts enables them to play a promising role in abusive language
  detection. However, various factors can affect their sensitivity. In this work,
  we test whether LLMs have an unintended bias in abusive language detection, i.e.,
  whether they predict more or less of a given abusive class than expected in zero-shot
  settings. Our results show that instruction-tuned LLMs tend to under-predict positive
  classes, since datasets used for tuning are dominated by the negative class. On
  the contrary, models fine-tuned with human feedback tend to be overly sensitive.
  In an exploratory approach to mitigate these issues, we show that label frequency
  in the prompt helps with the significant over-prediction.
links:
- name: URL
  url: https://aclanthology.org/2025.coling-main.188/
---
